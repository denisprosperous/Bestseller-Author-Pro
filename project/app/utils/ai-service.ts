import type { AIProvider } from "~/data/ai-providers";
import { aiCacheService } from "~/services/cache-service";

export interface AIRequest {
  provider: string;
  model: string;
  prompt: string;
  maxTokens?: number;
  temperature?: number;
  apiKey: string;
}

export interface AIResponse {
  content: string;
  provider: string;
  model: string;
  tokensUsed?: number;
}

interface ChatMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

/**
 * AI Service abstraction layer
 * Handles routing to different AI providers
 */
export class AIService {
  /**
   * Generate content using specified AI provider with caching and fallback support
   */
  async generateContent(request: AIRequest): Promise<AIResponse> {
    let { provider, model, prompt, maxTokens = 2000, temperature = 0.7, apiKey } = request;

    // Check cache first (only for deterministic requests with low temperature)
    if (temperature <= 0.3) {
      const cached = aiCacheService.getCachedAIResponse(provider, model, prompt);
      if (cached) {
        console.log(`‚úÖ Cache hit for ${provider}/${model}`);
        return {
          content: cached,
          provider,
          model,
          tokensUsed: 0 // Cached response doesn't use tokens
        };
      }
    }

    // Handle auto-selection with fallback
    if (provider === "auto") {
      return await this.generateWithFallback(prompt, maxTokens, temperature, apiKey);
    }

    // Auto-select model if needed - use best available model
    if (model === "auto") {
      model = await this.getBestAvailableModel(provider, apiKey);
    }

    // Validate API key with detailed error message
    const validationError = this.getApiKeyValidationError(provider, apiKey);
    if (validationError) {
      throw new Error(validationError);
    }

    // Call specific provider with retry logic
    const response = await this.callProviderWithRetry(provider, model, prompt, maxTokens, temperature, apiKey);
    
    // Cache successful response (only for deterministic requests)
    if (temperature <= 0.3) {
      aiCacheService.cacheAIResponse(provider, model, prompt, response.content, response.tokensUsed);
    }
    
    return response;
  }

  /**
   * Generate content with automatic provider fallback
   * Tries providers in preference order, skipping those without valid API keys
   */
  private async generateWithFallback(
    prompt: string,
    maxTokens: number,
    temperature: number,
    apiKey: string
  ): Promise<AIResponse> {
    const preferredOrder = ["openai", "anthropic", "xai", "google", "deepseek"];
    const errors: Array<{ provider: string; error: string; isTransient: boolean }> = [];

    console.log('üîÑ Starting auto-provider selection with fallback...');

    for (const provider of preferredOrder) {
      try {
        // Check if we have a valid API key for this provider
        const hasValidKey = await this.checkProviderAvailability(provider, apiKey);
        if (!hasValidKey) {
          console.log(`‚è≠Ô∏è  Skipping ${provider}: No valid API key`);
          errors.push({
            provider,
            error: 'No valid API key configured',
            isTransient: false
          });
          continue;
        }

        console.log(`üîÑ Trying provider: ${provider}`);
        
        // Get best available model for this provider
        const model = await this.getBestAvailableModel(provider, apiKey);
        
        // Try to generate content with retry logic
        const response = await this.callProviderWithRetry(provider, model, prompt, maxTokens, temperature, apiKey);
        
        console.log(`‚úÖ Successfully generated content using ${provider}/${model}`);
        
        // Cache successful response
        if (temperature <= 0.3) {
          aiCacheService.cacheAIResponse(provider, model, prompt, response.content, response.tokensUsed);
        }
        
        return response;
      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        const isTransient = this.isTransientError(errorMessage);
        
        console.warn(`‚ùå Provider ${provider} failed: ${errorMessage} (transient: ${isTransient})`);
        
        errors.push({
          provider,
          error: errorMessage,
          isTransient
        });
        
        // Continue to next provider
        continue;
      }
    }

    // If all providers failed, provide detailed error information
    const permanentFailures = errors.filter(e => !e.isTransient);
    const transientFailures = errors.filter(e => e.isTransient);
    
    let errorMessage = 'All AI providers failed. ';
    
    if (permanentFailures.length > 0) {
      errorMessage += `Permanent failures: ${permanentFailures.map(e => `${e.provider} (${e.error})`).join(', ')}. `;
    }
    
    if (transientFailures.length > 0) {
      errorMessage += `Transient failures: ${transientFailures.map(e => `${e.provider} (${e.error})`).join(', ')}. `;
      errorMessage += 'Please try again in a few moments.';
    } else {
      errorMessage += 'Please check your API keys in Settings.';
    }
    
    throw new Error(errorMessage);
  }

  /**
   * Call provider with exponential backoff retry logic for transient failures
   */
  private async callProviderWithRetry(
    provider: string,
    model: string,
    prompt: string,
    maxTokens: number,
    temperature: number,
    apiKey: string,
    maxRetries: number = 3
  ): Promise<AIResponse> {
    let lastError: Error | null = null;
    
    for (let attempt = 0; attempt < maxRetries; attempt++) {
      try {
        if (attempt > 0) {
          // Exponential backoff: 1s, 2s, 4s
          const delayMs = Math.pow(2, attempt) * 1000;
          console.log(`‚è≥ Retry attempt ${attempt + 1}/${maxRetries} for ${provider} after ${delayMs}ms delay`);
          await this.delay(delayMs);
        }
        
        return await this.callProvider(provider, model, prompt, maxTokens, temperature, apiKey);
      } catch (error) {
        lastError = error instanceof Error ? error : new Error('Unknown error');
        
        // Check if error is transient (worth retrying)
        if (!this.isTransientError(lastError.message)) {
          // Permanent error, don't retry
          throw lastError;
        }
        
        // Transient error, continue to next retry
        console.warn(`‚ö†Ô∏è  Transient error on attempt ${attempt + 1}: ${lastError.message}`);
      }
    }
    
    // All retries exhausted
    throw new Error(`Failed after ${maxRetries} retries: ${lastError?.message || 'Unknown error'}`);
  }

  /**
   * Check if a provider is available (has valid API key)
   */
  private async checkProviderAvailability(provider: string, apiKey: string): Promise<boolean> {
    // For auto mode, we need to check if the API key exists in localStorage
    // Since we're in the browser, we can use the localAPIKeyService
    try {
      // Import the service dynamically to avoid circular dependencies
      const { localAPIKeyService } = await import('~/services/local-api-key-service');
      const providerKey = localAPIKeyService.getApiKey(provider);
      
      if (!providerKey) {
        return false;
      }
      
      return this.validateApiKey(provider, providerKey);
    } catch (error) {
      console.warn(`Failed to check availability for ${provider}:`, error);
      return false;
    }
  }

  /**
   * Determine if an error is transient (worth retrying)
   */
  private isTransientError(errorMessage: string): boolean {
    const transientPatterns = [
      'rate limit',
      'rate_limit',
      '429',
      'timeout',
      'timed out',
      'network',
      'ECONNRESET',
      'ETIMEDOUT',
      'ENOTFOUND',
      'fetch failed',
      'service unavailable',
      '503',
      'temporarily unavailable',
      'overloaded',
      'too many requests'
    ];
    
    const lowerMessage = errorMessage.toLowerCase();
    return transientPatterns.some(pattern => lowerMessage.includes(pattern.toLowerCase()));
  }

  /**
   * Delay helper for exponential backoff
   */
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Call specific provider with error handling
   */
  private async callProvider(provider: string, model: string, prompt: string, maxTokens: number, temperature: number, apiKey: string): Promise<AIResponse> {
    switch (provider) {
      case "openai":
        return this.callOpenAI({ model, prompt, maxTokens, temperature, apiKey });
      case "anthropic":
        return this.callAnthropic({ model, prompt, maxTokens, temperature, apiKey });
      case "google":
        return this.callGoogle({ model, prompt, maxTokens, temperature, apiKey });
      case "xai":
        return this.callXAI({ model, prompt, maxTokens, temperature, apiKey });
      case "deepseek":
        return this.callDeepSeek({ model, prompt, maxTokens, temperature, apiKey });
      default:
        throw new Error(`Unsupported AI provider: ${provider}`);
    }
  }

  /**
   * Get best available model for a provider based on API key capabilities
   */
  private async getBestAvailableModel(provider: string, apiKey: string): Promise<string> {
    // Try to detect available models dynamically
    try {
      const availableModels = await this.getAvailableModels(provider, apiKey);
      if (availableModels.length > 0) {
        return this.selectBestModel(provider, availableModels);
      }
    } catch (error) {
      console.warn(`Failed to detect models for ${provider}, using defaults:`, error);
    }

    // Fallback to latest known models (2026)
    const latestModels: Record<string, string> = {
      openai: "gpt-5.2", // Latest GPT-5.2 (Jan 2026)
      anthropic: "claude-4-opus", // Latest Claude 4 Opus
      google: "gemini-2-pro", // Latest Gemini 2 Pro
      xai: "grok-3", // Latest Grok 3
      deepseek: "deepseek-llm-7b-instruct",
    };
    return latestModels[provider] || "gpt-5.2";
  }

  /**
   * Get default model for a provider (backwards compatibility)
   */
  private getDefaultModel(provider: string): string {
    const defaults: Record<string, string> = {
      openai: "gpt-5.2",
      anthropic: "claude-4-opus",
      google: "gemini-2-pro",
      xai: "grok-3",
      deepseek: "deepseek-llm-7b-instruct",
    };
    return defaults[provider] || "gpt-5.2";
  }

  /**
   * Get available models for a provider
   */
  private async getAvailableModels(provider: string, apiKey: string): Promise<string[]> {
    switch (provider) {
      case "openai":
        return this.getOpenAIModels(apiKey);
      case "anthropic":
        return this.getAnthropicModels(apiKey);
      case "google":
        return this.getGoogleModels(apiKey);
      case "xai":
        return this.getXAIModels(apiKey);
      case "deepseek":
        return this.getDeepSeekModels(apiKey);
      default:
        return [];
    }
  }

  /**
   * Select the best model from available models
   */
  private selectBestModel(provider: string, availableModels: string[]): string {
    // Define model preference order (best to fallback) - Updated for 2026
    const modelPreferences: Record<string, string[]> = {
      openai: [
        "gpt-5.2",
        "gpt-5",
        "gpt-4-turbo-2024-04-09",
        "gpt-4-turbo",
        "gpt-4-0125-preview",
        "gpt-4-1106-preview", 
        "gpt-4",
        "gpt-3.5-turbo-0125",
        "gpt-3.5-turbo"
      ],
      anthropic: [
        "claude-4-opus",
        "claude-4-sonnet",
        "claude-3-5-sonnet-20241022",
        "claude-3-5-sonnet-20240620",
        "claude-3-opus-20240229",
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307"
      ],
      google: [
        "gemini-2-pro",
        "gemini-2-flash",
        "gemini-1.5-pro-latest",
        "gemini-1.5-pro",
        "gemini-1.5-flash-latest",
        "gemini-1.5-flash",
        "gemini-pro"
      ],
      xai: [
        "grok-3",
        "grok-2-latest",
        "grok-beta"
      ],
      deepseek: [
        "deepseek-llm-7b-instruct",
        "deepseek-coder-7b-instruct"
      ]
    };

    const preferences = modelPreferences[provider] || [];
    
    // Find the first preferred model that's available
    for (const preferredModel of preferences) {
      if (availableModels.includes(preferredModel)) {
        return preferredModel;
      }
    }

    // If no preferred model found, return the first available
    return availableModels[0] || this.getDefaultModel(provider);
  }

  /**
   * Get available OpenAI models
   */
  private async getOpenAIModels(apiKey: string): Promise<string[]> {
    try {
      const response = await fetch("https://api.openai.com/v1/models", {
        headers: {
          Authorization: `Bearer ${apiKey}`,
        },
      });

      if (!response.ok) {
        throw new Error(`OpenAI models API error: ${response.statusText}`);
      }

      const data = await response.json();
      return data.data
        .filter((model: any) => model.id.includes('gpt'))
        .map((model: any) => model.id)
        .sort((a: string, b: string) => b.localeCompare(a)); // Sort newest first
    } catch (error) {
      console.warn('Failed to fetch OpenAI models:', error);
      return [];
    }
  }

  /**
   * Get available Anthropic models
   */
  private async getAnthropicModels(apiKey: string): Promise<string[]> {
    // Anthropic doesn't have a public models endpoint, return known models (2026)
    return [
      "claude-4-opus",
      "claude-4-sonnet",
      "claude-3-5-sonnet-20241022",
      "claude-3-5-sonnet-20240620", 
      "claude-3-opus-20240229",
      "claude-3-sonnet-20240229",
      "claude-3-haiku-20240307"
    ];
  }

  /**
   * Get available Google models
   */
  private async getGoogleModels(apiKey: string): Promise<string[]> {
    try {
      const response = await fetch(
        `https://generativelanguage.googleapis.com/v1beta/models?key=${apiKey}`,
        {
          headers: {
            "Content-Type": "application/json",
          },
        }
      );

      if (!response.ok) {
        throw new Error(`Google models API error: ${response.statusText}`);
      }

      const data = await response.json();
      return data.models
        .filter((model: any) => model.name.includes('gemini') && model.supportedGenerationMethods?.includes('generateContent'))
        .map((model: any) => model.name.replace('models/', ''))
        .sort((a: string, b: string) => b.localeCompare(a));
    } catch (error) {
      console.warn('Failed to fetch Google models:', error);
      return [];
    }
  }

  /**
   * Get available xAI models
   */
  private async getXAIModels(apiKey: string): Promise<string[]> {
    // xAI doesn't have a public models endpoint, return known models (2026)
    return [
      "grok-3",
      "grok-2-latest",
      "grok-beta"
    ];
  }

  /**
   * Get available DeepSeek models
   */
  private async getDeepSeekModels(apiKey: string): Promise<string[]> {
    // DeepSeek via Hugging Face, return known models
    return [
      "deepseek-llm-7b-instruct",
      "deepseek-coder-7b-instruct"
    ];
  }

  /**
   * Call OpenAI API
   */
  private async callOpenAI(params: {
    model: string;
    prompt: string;
    maxTokens: number;
    temperature: number;
    apiKey: string;
  }): Promise<AIResponse> {
    const messages: ChatMessage[] = [
      { role: "system", content: "You are a professional ebook author and writing assistant." },
      { role: "user", content: params.prompt },
    ];

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${params.apiKey}`,
      },
      body: JSON.stringify({
        model: params.model,
        messages,
        max_tokens: params.maxTokens,
        temperature: params.temperature,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`OpenAI API error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.choices[0].message.content,
      provider: "openai",
      model: params.model,
      tokensUsed: data.usage?.total_tokens,
    };
  }

  /**
   * Call Anthropic Claude API
   */
  private async callAnthropic(params: {
    model: string;
    prompt: string;
    maxTokens: number;
    temperature: number;
    apiKey: string;
  }): Promise<AIResponse> {
    const response = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "x-api-key": params.apiKey,
        "anthropic-version": "2023-06-01",
      },
      body: JSON.stringify({
        model: params.model,
        max_tokens: params.maxTokens,
        temperature: params.temperature,
        messages: [{ role: "user", content: params.prompt }],
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`Anthropic API error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.content[0].text,
      provider: "anthropic",
      model: params.model,
      tokensUsed: data.usage?.input_tokens + data.usage?.output_tokens,
    };
  }

  /**
   * Call Google Gemini API
   */
  private async callGoogle(params: {
    model: string;
    prompt: string;
    maxTokens: number;
    temperature: number;
    apiKey: string;
  }): Promise<AIResponse> {
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/${params.model}:generateContent?key=${params.apiKey}`,
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          contents: [
            {
              parts: [{ text: params.prompt }],
            },
          ],
          generationConfig: {
            temperature: params.temperature,
            maxOutputTokens: params.maxTokens,
          },
        }),
      },
    );

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`Google API error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    
    // Check if response has the expected structure
    if (!data.candidates || !data.candidates[0] || !data.candidates[0].content || !data.candidates[0].content.parts || !data.candidates[0].content.parts[0]) {
      console.error('Google API unexpected response:', JSON.stringify(data, null, 2));
      throw new Error(`Google API returned unexpected response format. This may be due to an invalid API key or API quota exceeded.`);
    }
    
    return {
      content: data.candidates[0].content.parts[0].text,
      provider: "google",
      model: params.model,
      tokensUsed: data.usageMetadata?.totalTokenCount,
    };
  }

  /**
   * Call xAI Grok API
   */
  private async callXAI(params: {
    model: string;
    prompt: string;
    maxTokens: number;
    temperature: number;
    apiKey: string;
  }): Promise<AIResponse> {
    const messages: ChatMessage[] = [
      { role: "system", content: "You are a professional ebook author and writing assistant." },
      { role: "user", content: params.prompt },
    ];

    const response = await fetch("https://api.x.ai/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${params.apiKey}`,
      },
      body: JSON.stringify({
        model: params.model,
        messages,
        max_tokens: params.maxTokens,
        temperature: params.temperature,
        stream: false,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`xAI API error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.choices[0].message.content,
      provider: "xai",
      model: params.model,
      tokensUsed: data.usage?.total_tokens,
    };
  }

  /**
   * Call DeepSeek API via Hugging Face
   */
  private async callDeepSeek(params: {
    model: string;
    prompt: string;
    maxTokens: number;
    temperature: number;
    apiKey: string;
  }): Promise<AIResponse> {
    const response = await fetch("https://api-inference.huggingface.co/models/deepseek-ai/deepseek-llm-7b-instruct", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${params.apiKey}`,
      },
      body: JSON.stringify({
        inputs: params.prompt,
        parameters: {
          max_new_tokens: params.maxTokens,
          temperature: params.temperature,
          return_full_text: false,
        },
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`DeepSeek API error: ${error.error || response.statusText}`);
    }

    const data = await response.json();
    const content = Array.isArray(data) ? data[0].generated_text : data.generated_text;

    return {
      content,
      provider: "deepseek",
      model: params.model,
    };
  }

  /**
   * Validate API key format for a provider with comprehensive checks
   */
  private validateApiKey(provider: string, apiKey: string): boolean {
    if (!apiKey || apiKey.trim().length === 0) {
      return false;
    }

    // Remove whitespace
    apiKey = apiKey.trim();

    // Basic format validation with detailed checks
    switch (provider) {
      case "openai":
        // OpenAI keys start with sk- and are typically 51 characters
        return apiKey.startsWith("sk-") && apiKey.length >= 20;
      case "anthropic":
        // Anthropic keys start with sk-ant- and are longer
        return apiKey.startsWith("sk-ant-") && apiKey.length >= 30;
      case "google":
        // Google API keys are typically 39 characters and alphanumeric
        return apiKey.length >= 20 && /^[A-Za-z0-9_-]+$/.test(apiKey);
      case "xai":
        // xAI keys start with xai- 
        return apiKey.startsWith("xai-") && apiKey.length >= 20;
      case "deepseek":
        // Hugging Face tokens start with hf_
        return apiKey.startsWith("hf_") && apiKey.length >= 20;
      default:
        // For unknown providers, just check it's not empty and has reasonable length
        return apiKey.length >= 10;
    }
  }

  /**
   * Get detailed validation error message for API key
   */
  getApiKeyValidationError(provider: string, apiKey: string): string | null {
    if (!apiKey || apiKey.trim().length === 0) {
      return `API key is required for ${provider}. Please add your API key in Settings.`;
    }

    apiKey = apiKey.trim();

    switch (provider) {
      case "openai":
        if (!apiKey.startsWith("sk-")) {
          return "OpenAI API keys must start with 'sk-'. Please check your key format.";
        }
        if (apiKey.length < 20) {
          return "OpenAI API key appears too short. Please verify your complete key.";
        }
        break;
      case "anthropic":
        if (!apiKey.startsWith("sk-ant-")) {
          return "Anthropic API keys must start with 'sk-ant-'. Please check your key format.";
        }
        if (apiKey.length < 30) {
          return "Anthropic API key appears too short. Please verify your complete key.";
        }
        break;
      case "google":
        if (apiKey.length < 20) {
          return "Google API key appears too short. Please verify your complete key.";
        }
        if (!/^[A-Za-z0-9_-]+$/.test(apiKey)) {
          return "Google API key contains invalid characters. Should only contain letters, numbers, underscores, and hyphens.";
        }
        break;
      case "xai":
        if (!apiKey.startsWith("xai-")) {
          return "xAI API keys must start with 'xai-'. Please check your key format.";
        }
        if (apiKey.length < 20) {
          return "xAI API key appears too short. Please verify your complete key.";
        }
        break;
      case "deepseek":
        if (!apiKey.startsWith("hf_")) {
          return "DeepSeek requires a Hugging Face token starting with 'hf_'. Please check your key format.";
        }
        if (apiKey.length < 20) {
          return "Hugging Face token appears too short. Please verify your complete token.";
        }
        break;
      default:
        if (apiKey.length < 10) {
          return `API key for ${provider} appears too short. Please verify your complete key.`;
        }
    }

    return null; // Key is valid
  }

  /**
   * Test API key by making a minimal request
   */
  async testApiKey(provider: string, apiKey: string): Promise<{ valid: boolean; error?: string }> {
    // First check format
    const formatError = this.getApiKeyValidationError(provider, apiKey);
    if (formatError) {
      return { valid: false, error: formatError };
    }

    try {
      // Make a minimal test request
      const testPrompt = "Hello";
      await this.generateContent({
        provider,
        model: this.getDefaultModel(provider),
        prompt: testPrompt,
        apiKey,
        maxTokens: 10,
        temperature: 0.1
      });

      return { valid: true };
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      
      // Parse common API errors
      if (errorMessage.includes('401') || errorMessage.includes('unauthorized')) {
        return { valid: false, error: `Invalid API key for ${provider}. Please check your key is correct and active.` };
      }
      if (errorMessage.includes('403') || errorMessage.includes('forbidden')) {
        return { valid: false, error: `API key for ${provider} lacks required permissions or has exceeded quota.` };
      }
      if (errorMessage.includes('429') || errorMessage.includes('rate limit')) {
        return { valid: false, error: `Rate limit exceeded for ${provider}. API key is valid but temporarily blocked.` };
      }
      if (errorMessage.includes('network') || errorMessage.includes('fetch')) {
        return { valid: false, error: `Network error testing ${provider} API key. Please check your connection.` };
      }

      return { valid: false, error: `Failed to validate ${provider} API key: ${errorMessage}` };
    }
  }

  /**
   * Check if API key exists for auto provider selection
   */
  private async checkApiKeyExists(provider: string, apiKey: string): Promise<boolean> {
    return this.validateApiKey(provider, apiKey);
  }

  /**
   * Improve an outline using AI with real API calls
   */
  async improveOutline(outline: string, provider: string, model: string, apiKey: string): Promise<string> {
    if (!this.validateApiKey(provider, apiKey)) {
      throw new Error(`Invalid API key for ${provider}. Please check your API key in settings.`);
    }

    const prompt = `You are a bestselling author and book structure expert. Analyze and improve this book outline to make it more comprehensive, engaging, and marketable.

Current Outline:
${outline}

Instructions:
- Enhance chapter titles to be more compelling and specific
- Add missing chapters that would strengthen the book
- Improve the logical flow and progression
- Add 3-5 section topics under each chapter
- Ensure the structure appeals to readers and provides clear value
- Maintain the core topic and intent
- Return the improved outline in a clear, structured format

Provide the enhanced outline:`;

    try {
      const response = await this.generateContent({
        provider,
        model,
        prompt,
        apiKey,
        maxTokens: 2000,
        temperature: 0.6,
      });

      return response.content;
    } catch (error) {
      if (error instanceof Error && error.message.includes('API key')) {
        throw error;
      }
      throw new Error(`Failed to improve outline: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Generate brainstorming suggestions with real AI and caching
   * Supports auto-provider fallback
   */
  async brainstorm(topic: string, provider: string, model: string, apiKey: string): Promise<{ titles: string[]; outline: string }> {
    // For auto provider, apiKey is not used (we'll fetch from localStorage)
    if (provider !== "auto" && !this.validateApiKey(provider, apiKey)) {
      throw new Error(`Invalid API key for ${provider}. Please check your API key in settings.`);
    }

    // Check cache first (use provider-agnostic cache for auto mode)
    const cacheKey = provider === "auto" ? "auto" : provider;
    const cached = aiCacheService.getCachedBrainstorm(topic, cacheKey);
    if (cached) {
      console.log(`‚úÖ Cache hit for brainstorm: ${topic}`);
      return cached;
    }

    const prompt = `You are a bestselling ebook author and publishing expert. Generate 5 compelling, marketable book titles and a detailed chapter outline for a book about: ${topic}

Requirements:
- Titles should be attention-grabbing and marketable
- Outline should have 8-12 chapters with descriptive titles
- Each chapter should have 3-5 section topics
- Focus on practical, actionable content
- Consider SEO and keyword optimization

Return your response as JSON with this exact structure:
{
  "titles": ["Title 1", "Title 2", "Title 3", "Title 4", "Title 5"],
  "outline": {
    "title": "Main Book Title",
    "subtitle": "Compelling Subtitle",
    "chapters": [
      {
        "number": 1,
        "title": "Chapter Title",
        "sections": ["Section 1", "Section 2", "Section 3"]
      }
    ]
  }
}`;

    try {
      const response = await this.generateContent({
        provider,
        model,
        prompt,
        apiKey,
        temperature: 0.8,
        maxTokens: 3000
      });

      // Try to parse JSON response
      try {
        const parsed = JSON.parse(response.content);
        if (parsed.titles && parsed.outline) {
          const result = {
            titles: parsed.titles,
            outline: JSON.stringify(parsed.outline, null, 2)
          };
          
          // Cache the result (use actual provider from response)
          aiCacheService.cacheBrainstorm(topic, response.provider, result);
          return result;
        }
      } catch (parseError) {
        // If JSON parsing fails, return structured fallback
        console.warn('Failed to parse AI response as JSON, using fallback structure');
      }

      // Fallback: extract titles and create basic outline
      const lines = response.content.split('\n').filter(line => line.trim());
      const titles = lines
        .filter(line => line.includes('Title') || line.match(/^\d+\./))
        .slice(0, 5)
        .map(line => line.replace(/^\d+\.?\s*/, '').replace(/Title\s*\d*:?\s*/i, '').trim());

      if (titles.length === 0) {
        titles.push(
          `The Complete Guide to ${topic}`,
          `Mastering ${topic}: A Practical Approach`,
          `${topic} for Beginners and Experts`,
          `The Ultimate ${topic} Handbook`,
          `Transform Your Life with ${topic}`
        );
      }

      const result = {
        titles,
        outline: response.content
      };

      // Cache the fallback result
      aiCacheService.cacheBrainstorm(topic, response.provider, result);
      return result;
    } catch (error) {
      if (error instanceof Error && error.message.includes('API key')) {
        throw error;
      }
      throw new Error(`Failed to generate brainstorm ideas: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Generate complete ebook content with real AI
   * Supports auto-provider fallback
   */
  async generateEbook(params: {
    topic: string;
    wordCount: number;
    tone: string;
    customTone?: string;
    audience: string;
    outline?: string;
    provider: string;
    model: string;
    apiKey: string;
  }): Promise<string> {
    // For auto provider, apiKey is not used (we'll fetch from localStorage)
    if (params.provider !== "auto" && !this.validateApiKey(params.provider, params.apiKey)) {
      throw new Error(`Invalid API key for ${params.provider}. Please check your API key in settings.`);
    }

    const outlineSection = params.outline ? `\n\nUse this outline as your structure:\n${params.outline}` : "";
    const toneInstruction = params.tone === "custom" && params.customTone
      ? `Writing style: ${params.customTone}`
      : `Tone: ${params.tone}`;

    const prompt = `You are a professional ghostwriter and bestselling author. Write a complete, detailed ebook on the topic: ${params.topic}.

Requirements:
- Target word count: ${params.wordCount} words
- ${toneInstruction}
- Target audience: ${params.audience}
- Include: Title Page, Table of Contents, Introduction, Multiple Chapters (8-15), Conclusion, Resources/References
- Format in clean Markdown with proper heading structure
- Use H1 for main title, H2 for major sections, H3 for chapters
- Write engaging, valuable content that provides real insights
- Include practical examples, actionable advice, and clear explanations
- Ensure content flows logically from chapter to chapter${outlineSection}

Generate the complete ebook content now in Markdown format.`;

    try {
      const response = await this.generateContent({
        provider: params.provider,
        model: params.model,
        prompt,
        apiKey: params.apiKey,
        maxTokens: Math.min(8000, Math.floor(params.wordCount / 2)), // Rough token estimation
        temperature: 0.7,
      });

      return response.content;
    } catch (error) {
      if (error instanceof Error && error.message.includes('API key')) {
        throw error;
      }
      throw new Error(`Failed to generate ebook: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Generate a single chapter with real AI
   * Supports auto-provider fallback
   */
  async generateChapter(params: {
    chapterTitle: string;
    chapterNumber: number;
    outline: string;
    tone: string;
    audience: string;
    wordCount: number;
    provider: string;
    model: string;
    apiKey: string;
  }): Promise<string> {
    // For auto provider, apiKey is not used (we'll fetch from localStorage)
    if (params.provider !== "auto" && !this.validateApiKey(params.provider, params.apiKey)) {
      throw new Error(`Invalid API key for ${params.provider}. Please check your API key in settings.`);
    }

    const prompt = `You are a professional author writing Chapter ${params.chapterNumber} of a book. 

Chapter Details:
- Title: "${params.chapterTitle}"
- Target word count: ${params.wordCount} words
- Tone: ${params.tone}
- Audience: ${params.audience}

Book Context:
${params.outline}

Instructions:
- Write a complete, engaging chapter that fits within the overall book structure
- Include practical examples, actionable advice, and clear explanations
- Use subheadings to organize content (H3 and H4 levels)
- Ensure the chapter flows well and provides real value
- Write in Markdown format
- Start with the chapter title as H2: ## Chapter ${params.chapterNumber}: ${params.chapterTitle}

Generate the complete chapter content now.`;

    try {
      const response = await this.generateContent({
        provider: params.provider,
        model: params.model,
        prompt,
        apiKey: params.apiKey,
        maxTokens: Math.min(4000, Math.floor(params.wordCount / 2)),
        temperature: 0.7,
      });

      return response.content;
    } catch (error) {
      if (error instanceof Error && error.message.includes('API key')) {
        throw error;
      }
      throw new Error(`Failed to generate chapter: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Humanize content using real AI processing with caching
   * Supports auto-provider fallback
   */
  async humanizeContent(content: string, provider: string, model: string, apiKey: string): Promise<string> {
    // For auto provider, apiKey is not used (we'll fetch from localStorage)
    if (provider !== "auto" && !this.validateApiKey(provider, apiKey)) {
      throw new Error(`Invalid API key for ${provider}. Please check your API key in settings.`);
    }

    // Check cache first (use provider-agnostic cache for auto mode)
    const cacheKey = provider === "auto" ? "auto" : provider;
    const cached = aiCacheService.getCachedHumanization(content, cacheKey);
    if (cached) {
      console.log(`‚úÖ Cache hit for humanization`);
      return cached;
    }

    const prompt = `You are an expert editor specializing in making AI-generated content sound more natural and human-written. Transform this content through a comprehensive humanization process:

CONTENT TO HUMANIZE:
${content}

HUMANIZATION REQUIREMENTS:
1. STRUCTURAL REWRITE: Vary sentence lengths, merge short sentences, break up long ones
2. LEXICAL IMPROVEMENTS: Replace AI clich√©s and robotic phrases with natural language
3. PERSONALITY INJECTION: Add contractions, personal touches, conversational elements
4. FLOW ENHANCEMENT: Improve transitions and logical connections between ideas

BANNED AI PHRASES TO REPLACE:
- "delve into" ‚Üí "explore" or "examine"
- "leverage" ‚Üí "use" or "apply"
- "tapestry" ‚Üí "mix" or "blend"
- "navigate" ‚Üí "handle" or "manage"
- "robust" ‚Üí "strong" or "solid"
- "pivotal" ‚Üí "key" or "important"
- "moreover" ‚Üí "also" or "plus"
- "furthermore" ‚Üí "what's more" or "additionally"

STYLE GUIDELINES:
- Use contractions (don't, can't, it's, you're)
- Add occasional rhetorical questions
- Include personal pronouns (you, we, I)
- Vary paragraph lengths
- Use active voice over passive
- Add transitional phrases that sound natural

Return the fully humanized content that reads as if written by a skilled human author.`;

    try {
      const response = await this.generateContent({
        provider,
        model,
        prompt,
        apiKey,
        maxTokens: Math.min(4000, content.length * 2), // Allow for expansion
        temperature: 0.3, // Lower temperature for more consistent editing
      });

      // Cache the result (use actual provider from response)
      aiCacheService.cacheHumanization(content, response.provider, response.content);
      
      return response.content;
    } catch (error) {
      if (error instanceof Error && error.message.includes('API key')) {
        throw error;
      }
      throw new Error(`Failed to humanize content: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
}

export const aiService = new AIService();
